"""
ç”Ÿæˆ SenseVoice è®­ç»ƒæŠ¥å‘Š

åŠŸèƒ½ï¼š
1. åˆ†æè®­ç»ƒé…ç½®å’Œæ£€æŸ¥ç‚¹
2. ç»Ÿè®¡è®­ç»ƒæ•°æ®é›†ä¿¡æ¯
3. æ£€æŸ¥ TensorBoard æ—¥å¿—
4. ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è®­ç»ƒæŠ¥å‘Š

ä¾èµ–ï¼špip install pyyaml
"""

import os
import json
import yaml
from pathlib import Path
from datetime import datetime


def get_file_size_mb(file_path):
    """è·å–æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰"""
    if os.path.exists(file_path):
        return os.path.getsize(file_path) / (1024 * 1024)
    return 0


def analyze_checkpoints(output_dir):
    """åˆ†ææ¨¡å‹æ£€æŸ¥ç‚¹"""
    checkpoints = []
    for file in Path(output_dir).glob("model.pt*"):
        checkpoints.append({
            'name': file.name,
            'size_mb': round(get_file_size_mb(file), 2),
            'modified': datetime.fromtimestamp(file.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')
        })
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    checkpoints.sort(key=lambda x: x['modified'], reverse=True)
    return checkpoints


def count_samples(jsonl_path):
    """ç»Ÿè®¡ JSONL æ–‡ä»¶æ ·æœ¬æ•°"""
    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip()]
            return len(lines)
    except FileNotFoundError:
        return 0


def analyze_data_distribution(jsonl_path, sample_limit=5):
    """åˆ†ææ•°æ®åˆ†å¸ƒ"""
    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            samples = []
            for line in f:
                line = line.strip()
                if line:
                    try:
                        data = json.loads(line)
                        samples.append({
                            'key': data.get('key', 'N/A'),
                            'target_len': data.get('target_len', 0),
                            'target': data.get('target', '')[:50]  # å‰50å­—ç¬¦
                        })
                    except json.JSONDecodeError:
                        continue
            
            if samples:
                avg_len = sum(s['target_len'] for s in samples) / len(samples)
                return {
                    'total': len(samples),
                    'avg_target_len': round(avg_len, 2),
                    'samples': samples[:sample_limit]
                }
    except FileNotFoundError:
        pass
    
    return {'total': 0, 'avg_target_len': 0, 'samples': []}


def load_config(config_path):
    """åŠ è½½è®­ç»ƒé…ç½®"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        return {}


def generate_report(output_dir="../../exp_svs", train_data="../../data/list/train_from_excel.jsonl", 
                   val_data="../../data/list/val_from_excel.jsonl"):
    """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
    
    # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶åï¼ˆæ—¶é—´æˆ³ï¼Œè¾“å‡ºåˆ°é¡¹ç›®æ ¹ç›®å½•ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = f"../../training_report_{timestamp}.md"
    
    # æ”¶é›†ä¿¡æ¯
    config = load_config(os.path.join(output_dir, "config.yaml"))
    checkpoints = analyze_checkpoints(output_dir)
    train_info = analyze_data_distribution(train_data)
    val_info = analyze_data_distribution(val_data)
    
    # ç”ŸæˆæŠ¥å‘Šå†…å®¹
    report = f"""# SenseVoice è®­ç»ƒæŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š è®­ç»ƒæ¦‚è§ˆ

### æ•°æ®é›†ç»Ÿè®¡

| ç±»å‹ | æ ·æœ¬æ•° | å¹³å‡æ–‡æœ¬é•¿åº¦ |
|------|--------|--------------|
| è®­ç»ƒé›† | {train_info['total']} | {train_info['avg_target_len']} å­—ç¬¦ |
| éªŒè¯é›† | {val_info['total']} | {val_info['avg_target_len']} å­—ç¬¦ |
| **æ€»è®¡** | **{train_info['total'] + val_info['total']}** | - |

### è®­ç»ƒé…ç½®

"""
    
    # æ·»åŠ è®­ç»ƒé…ç½®
    if config:
        train_conf = config.get('train_conf', {})
        dataset_conf = config.get('dataset_conf', {})
        optim_conf = config.get('optim_conf', {})
        
        report += f"""| å‚æ•° | å€¼ |
|------|-----|
| æœ€å¤§è½®æ•° (max_epoch) | {train_conf.get('max_epoch', 'N/A')} |
| æ‰¹æ¬¡å¤§å° (batch_size) | {dataset_conf.get('batch_size', 'N/A')} |
| æ‰¹æ¬¡ç±»å‹ (batch_type) | {dataset_conf.get('batch_type', 'N/A')} |
| å­¦ä¹ ç‡ (lr) | {optim_conf.get('lr', 'N/A')} |
| ä¼˜åŒ–å™¨ | {config.get('optim', 'N/A')} |
| è°ƒåº¦å™¨ | {config.get('scheduler', 'N/A')} |
| éªŒè¯é—´éš” | {train_conf.get('validate_interval', 'N/A')} æ­¥ |
| ä¿å­˜é—´éš” | {train_conf.get('save_checkpoint_interval', 'N/A')} æ­¥ |
| ä¿ç•™æœ€ä½³æ¨¡å‹æ•° | {train_conf.get('keep_nbest_models', 'N/A')} |

"""
    
    # æ¨¡å‹æ£€æŸ¥ç‚¹
    report += f"""---

## ğŸ¯ æ¨¡å‹æ£€æŸ¥ç‚¹

å…± {len(checkpoints)} ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶ï¼š

| æ–‡ä»¶å | å¤§å° (MB) | ä¿®æ”¹æ—¶é—´ |
|--------|-----------|----------|
"""
    
    for ckpt in checkpoints[:15]:  # åªæ˜¾ç¤ºå‰15ä¸ª
        report += f"| {ckpt['name']} | {ckpt['size_mb']} | {ckpt['modified']} |\n"
    
    if len(checkpoints) > 15:
        report += f"\n*...è¿˜æœ‰ {len(checkpoints) - 15} ä¸ªæ£€æŸ¥ç‚¹æœªæ˜¾ç¤º*\n"
    
    # è®­ç»ƒæ ·æœ¬ç¤ºä¾‹
    if train_info['samples']:
        report += f"""
---

## ğŸ“ è®­ç»ƒæ•°æ®æ ·ä¾‹

å‰ {len(train_info['samples'])} ä¸ªè®­ç»ƒæ ·æœ¬ï¼š

| ç¼–å· | Key | æ–‡æœ¬é•¿åº¦ | æ–‡æœ¬å†…å®¹ |
|------|-----|----------|----------|
"""
        for i, sample in enumerate(train_info['samples'], 1):
            report += f"| {i} | {sample['key']} | {sample['target_len']} | {sample['target'][:40]}... |\n"
    
    # é—®é¢˜è¯Šæ–­
    report += """
---

## âš ï¸ è®­ç»ƒè¯Šæ–­

### æ½œåœ¨é—®é¢˜

"""
    
    issues = []
    
    # æ£€æŸ¥æ•°æ®é‡
    if train_info['total'] < 100:
        issues.append("- âš ï¸ **è®­ç»ƒæ ·æœ¬è¿‡å°‘** - ä»… {} ä¸ªæ ·æœ¬ï¼Œå»ºè®®è‡³å°‘ 1000+ æ ·æœ¬ä»¥è·å¾—æ›´å¥½æ•ˆæœ".format(train_info['total']))
    
    # æ£€æŸ¥æ£€æŸ¥ç‚¹æ›´æ–°
    if checkpoints:
        latest_time = datetime.strptime(checkpoints[0]['modified'], '%Y-%m-%d %H:%M:%S')
        time_diff = datetime.now() - latest_time
        if time_diff.total_seconds() > 3600:  # è¶…è¿‡1å°æ—¶
            issues.append(f"- âš ï¸ **æ£€æŸ¥ç‚¹æœªæ›´æ–°** - æœ€æ–°æ£€æŸ¥ç‚¹äº {checkpoints[0]['modified']} ç”Ÿæˆï¼ˆ{round(time_diff.total_seconds()/3600, 1)} å°æ—¶å‰ï¼‰")
    
    # æ£€æŸ¥æ¨¡å‹å¤§å°å˜åŒ–
    if len(checkpoints) >= 2:
        size_diff = abs(checkpoints[0]['size_mb'] - checkpoints[1]['size_mb'])
        if size_diff < 1:  # å¤§å°å‡ ä¹æ— å˜åŒ–
            issues.append("- âš ï¸ **æ¨¡å‹å¤§å°æ— æ˜æ˜¾å˜åŒ–** - å¯èƒ½è®­ç»ƒæœªçœŸæ­£è¿›è¡Œæˆ–å·²æ”¶æ•›")
    
    if not issues:
        report += "âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜\n"
    else:
        report += "\n".join(issues) + "\n"
    
    # å»ºè®®
    report += """
### æ”¹è¿›å»ºè®®

1. **æ•°æ®è´¨é‡**
   - ç¡®ä¿éŸ³é¢‘æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®
   - æ£€æŸ¥æ–‡æœ¬æ ‡æ³¨çš„å‡†ç¡®æ€§
   - éªŒè¯æ•°æ®ç¼–ç æ ¼å¼ï¼ˆUTF-8ï¼‰

2. **è®­ç»ƒå‚æ•°**
   - æ ·æœ¬æ•°è¾ƒå°‘æ—¶ï¼Œé™ä½ batch_sizeï¼ˆå¦‚ 2000-3000ï¼‰
   - å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆmax_epoch 20-50ï¼‰
   - è°ƒæ•´å­¦ä¹ ç‡ï¼ˆ0.0001-0.0002ï¼‰

3. **ç›‘æ§è®­ç»ƒ**
   - ä½¿ç”¨ TensorBoard æŸ¥çœ‹ loss æ›²çº¿ï¼š`.\start_tensorboard.ps1`
   - æ£€æŸ¥ GPU åˆ©ç”¨ç‡æ˜¯å¦æ­£å¸¸
   - è§‚å¯ŸéªŒè¯é›†æŒ‡æ ‡å˜åŒ–

4. **æ•°æ®å¢å¼º**
   - æ”¶é›†æ›´å¤šè®­ç»ƒæ•°æ®
   - ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯ï¼ˆå¦‚è¯­é€Ÿå˜åŒ–ã€å™ªå£°æ·»åŠ ï¼‰
   - ç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†å¸ƒä¸€è‡´

---

## ğŸ“ æ–‡ä»¶ä½ç½®

- **è¾“å‡ºç›®å½•**: `{output_dir}`
- **è®­ç»ƒæ•°æ®**: `{train_data}`
- **éªŒè¯æ•°æ®**: `{val_data}`
- **é…ç½®æ–‡ä»¶**: `{os.path.join(output_dir, 'config.yaml')}`
- **TensorBoard**: `{os.path.join(output_dir, 'tensorboard')}`

---

## ğŸ”— ç›¸å…³å‘½ä»¤

```powershell
# æŸ¥çœ‹ TensorBoard
.\start_tensorboard.ps1

# ç»§ç»­è®­ç»ƒ
.\finetune_sensevoice_fixed.ps1 -MaxEpoch 20

# æ¸…é™¤æ—§æ£€æŸ¥ç‚¹é‡æ–°è®­ç»ƒ
Remove-Item {output_dir}\model.pt* -Force
.\finetune_sensevoice_fixed.ps1
```

---

*æŠ¥å‘Šç”Ÿæˆäº: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    # å†™å…¥æŠ¥å‘Š
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… è®­ç»ƒæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {train_info['total']} | éªŒè¯æ ·æœ¬: {val_info['total']}")
    print(f"ğŸ“ æ£€æŸ¥ç‚¹æ–‡ä»¶: {len(checkpoints)} ä¸ª")
    
    return report_path


if __name__ == "__main__":
    generate_report()
