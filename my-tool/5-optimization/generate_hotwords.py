#!/usr/bin/env python3
"""
ä»è®­ç»ƒæ•°æ®ä¸­æå–çƒ­è¯å¹¶ç”Ÿæˆçƒ­è¯æ–‡ä»¶
ç”¨äºæå‡ ONNX æ¨¡å‹æ¨ç†å‡†ç¡®ç‡
"""

import json
from pathlib import Path
from collections import Counter
import jieba
import re


def load_jsonl(jsonl_path):
    """åŠ è½½JSONLæ•°æ®"""
    data = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line.strip())
            data.append(item)
    return data


def extract_words_and_phrases(data, min_freq=2, max_length=10):
    """
    ä»æ•°æ®ä¸­æå–é«˜é¢‘è¯æ±‡å’ŒçŸ­è¯­
    
    Args:
        data: æ•°æ®åˆ—è¡¨
        min_freq: æœ€å°å‡ºç°é¢‘ç‡
        max_length: æœ€å¤§çƒ­è¯é•¿åº¦
    
    Returns:
        çƒ­è¯åˆ—è¡¨åŠå…¶é¢‘ç‡
    """
    # ç»Ÿè®¡æ‰€æœ‰ç›®æ ‡æ–‡æœ¬
    all_texts = []
    for item in data:
        text = item.get('target', '')
        if text:
            all_texts.append(text)
    
    # ç»Ÿè®¡å®Œæ•´çŸ­è¯­çš„é¢‘ç‡
    phrase_counter = Counter()
    for text in all_texts:
        # æŒ‰ç©ºæ ¼åˆ†å‰²æˆçŸ­è¯­
        phrases = text.split()
        for phrase in phrases:
            if len(phrase) <= max_length and len(phrase) > 0:
                phrase_counter[phrase] += 1
    
    # ç»Ÿè®¡åˆ†è¯åçš„è¯è¯­é¢‘ç‡
    word_counter = Counter()
    for text in all_texts:
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.lcut(text)
        for word in words:
            # è¿‡æ»¤é•¿åº¦
            if 1 < len(word) <= max_length:
                word_counter[word] += 1
    
    # åˆå¹¶çŸ­è¯­å’Œè¯è¯­
    combined_counter = phrase_counter + word_counter
    
    # è¿‡æ»¤ä½é¢‘è¯
    hotwords = {word: freq for word, freq in combined_counter.items() if freq >= min_freq}
    
    return hotwords


def calculate_hotword_weight(freq, max_freq, min_weight=20, max_weight=100):
    """
    æ ¹æ®é¢‘ç‡è®¡ç®—çƒ­è¯æƒé‡
    
    Args:
        freq: å½“å‰è¯é¢‘
        max_freq: æœ€å¤§è¯é¢‘
        min_weight: æœ€å°æƒé‡
        max_weight: æœ€å¤§æƒé‡
    
    Returns:
        æƒé‡å€¼
    """
    if max_freq == 0:
        return min_weight
    
    # ä½¿ç”¨å¯¹æ•°scaleé¿å…æƒé‡å·®å¼‚è¿‡å¤§
    import math
    log_freq = math.log(freq + 1)
    log_max = math.log(max_freq + 1)
    
    normalized = log_freq / log_max
    weight = min_weight + (max_weight - min_weight) * normalized
    
    return int(weight)


def generate_hotword_file(hotwords, output_path, top_k=100):
    """
    ç”Ÿæˆçƒ­è¯æ–‡ä»¶
    
    Args:
        hotwords: {è¯: é¢‘ç‡} å­—å…¸
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        top_k: ä¿ç•™å‰Kä¸ªé«˜é¢‘è¯
    """
    # æŒ‰é¢‘ç‡æ’åº
    sorted_hotwords = sorted(hotwords.items(), key=lambda x: x[1], reverse=True)
    
    # å–å‰top_kä¸ª
    top_hotwords = sorted_hotwords[:top_k]
    
    # è®¡ç®—æƒé‡
    max_freq = top_hotwords[0][1] if top_hotwords else 1
    
    # å†™å…¥æ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        for word, freq in top_hotwords:
            weight = calculate_hotword_weight(freq, max_freq)
            f.write(f"{word} {weight}\n")
    
    print(f"\nâœ“ ç”Ÿæˆçƒ­è¯æ–‡ä»¶: {output_path}")
    print(f"  çƒ­è¯æ•°é‡: {len(top_hotwords)}")
    print(f"\nå‰10ä¸ªé«˜é¢‘çƒ­è¯:")
    for i, (word, freq) in enumerate(top_hotwords[:10], 1):
        weight = calculate_hotword_weight(freq, max_freq)
        print(f"  {i:2d}. {word:15s} (é¢‘ç‡: {freq:3d}, æƒé‡: {weight:3d})")


def analyze_error_patterns(comparison_report_path):
    """
    åˆ†ææ¨¡å‹é”™è¯¯æ¨¡å¼ï¼Œæå–å®¹æ˜“è¯†åˆ«é”™è¯¯çš„è¯
    
    Args:
        comparison_report_path: å¯¹æ¯”æŠ¥å‘Šè·¯å¾„
    
    Returns:
        å®¹æ˜“å‡ºé”™çš„è¯åˆ—è¡¨
    """
    error_words = set()
    
    try:
        with open(comparison_report_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # æŸ¥æ‰¾é”™è¯¯æ ·æœ¬
        # ç¤ºä¾‹: | 9 | 2025_11_17_13_30_57.wav | çº¢ç¯è·Ÿè½¦ | çº¢ç¯å¥”è½¦ | âœ— é”™è¯¯ |
        pattern = r'\| \d+ \| .+? \| (.+?) \| (.+?) \| âœ— é”™è¯¯ \|'
        matches = re.findall(pattern, content)
        
        for ground_truth, recognized in matches:
            # æå–ground_truthä¸­çš„è¯ï¼Œè¿™äº›æ˜¯å®¹æ˜“è¢«è¯†åˆ«é”™çš„
            words = jieba.lcut(ground_truth.strip())
            for word in words:
                if len(word) > 1:
                    error_words.add(word)
        
        print(f"\nä»é”™è¯¯æ ·æœ¬ä¸­æå–äº† {len(error_words)} ä¸ªå®¹æ˜“å‡ºé”™çš„è¯")
        print(f"ç¤ºä¾‹: {list(error_words)[:10]}")
    
    except FileNotFoundError:
        print(f"\nè­¦å‘Š: æœªæ‰¾åˆ°å¯¹æ¯”æŠ¥å‘Š {comparison_report_path}")
    
    return error_words


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä»è®­ç»ƒæ•°æ®ç”Ÿæˆçƒ­è¯æ–‡ä»¶')
    parser.add_argument('--train_data', type=str, default='../../data/list/train_from_excel.jsonl',
                        help='è®­ç»ƒæ•°æ®JSONLæ–‡ä»¶')
    parser.add_argument('--val_data', type=str, default='../../data/list/val_from_excel.jsonl',
                        help='éªŒè¯æ•°æ®JSONLæ–‡ä»¶')
    parser.add_argument('--output', type=str, default='../../exp_svs_onnx/hotwords.txt',
                        help='çƒ­è¯è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--top_k', type=int, default=100,
                        help='ä¿ç•™å‰Kä¸ªé«˜é¢‘è¯ (å»ºè®®ä¸è¶…è¿‡1000)')
    parser.add_argument('--min_freq', type=int, default=2,
                        help='æœ€å°è¯é¢‘')
    parser.add_argument('--comparison_report', type=str, default=None,
                        help='æ¨¡å‹å¯¹æ¯”æŠ¥å‘Šè·¯å¾„,ç”¨äºæå–å®¹æ˜“å‡ºé”™çš„è¯')
    
    args = parser.parse_args()
    
    print(f"\n{'='*80}")
    print("ç”Ÿæˆ ONNX çƒ­è¯æ–‡ä»¶")
    print(f"{'='*80}\n")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    all_data = []
    
    if Path(args.train_data).exists():
        print(f"åŠ è½½è®­ç»ƒæ•°æ®: {args.train_data}")
        train_data = load_jsonl(args.train_data)
        all_data.extend(train_data)
        print(f"  âœ“ è®­ç»ƒæ ·æœ¬: {len(train_data)}")
    
    if Path(args.val_data).exists():
        print(f"åŠ è½½éªŒè¯æ•°æ®: {args.val_data}")
        val_data = load_jsonl(args.val_data)
        all_data.extend(val_data)
        print(f"  âœ“ éªŒè¯æ ·æœ¬: {len(val_data)}")
    
    if not all_data:
        print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶")
        return
    
    print(f"\næ€»æ ·æœ¬æ•°: {len(all_data)}")
    
    # æå–çƒ­è¯
    print(f"\næå–çƒ­è¯ (æœ€å°é¢‘ç‡: {args.min_freq}, æœ€å¤§é•¿åº¦: 10)...")
    hotwords = extract_words_and_phrases(all_data, min_freq=args.min_freq)
    print(f"  âœ“ æå–äº† {len(hotwords)} ä¸ªå€™é€‰çƒ­è¯")
    
    # åˆ†æé”™è¯¯æ¨¡å¼(å¦‚æœæä¾›äº†å¯¹æ¯”æŠ¥å‘Š)
    error_words = set()
    if args.comparison_report and Path(args.comparison_report).exists():
        print(f"\nåˆ†æé”™è¯¯æ¨¡å¼: {args.comparison_report}")
        error_words = analyze_error_patterns(args.comparison_report)
        
        # æå‡å®¹æ˜“å‡ºé”™è¯çš„æƒé‡
        for word in error_words:
            if word in hotwords:
                hotwords[word] = hotwords[word] * 2  # åŠ å€æƒé‡
    
    # ç”Ÿæˆçƒ­è¯æ–‡ä»¶
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    generate_hotword_file(hotwords, output_path, top_k=args.top_k)
    
    # ç”Ÿæˆä½¿ç”¨è¯´æ˜
    print(f"\n{'='*80}")
    print("ä½¿ç”¨è¯´æ˜")
    print(f"{'='*80}\n")
    
    print("âš ï¸  æ³¨æ„äº‹é¡¹:")
    print("  1. SenseVoice-ONNX å¯¹çƒ­è¯çš„æ”¯æŒæœ‰é™")
    print("  2. å»ºè®®çƒ­è¯æ•°é‡ä¸è¶…è¿‡1000ä¸ª")
    print("  3. å»ºè®®çƒ­è¯é•¿åº¦ä¸è¶…è¿‡10ä¸ªå­—ç¬¦")
    print("  4. æƒé‡èŒƒå›´: 1-100 (æ•°å€¼è¶Šå¤§,ä¼˜å…ˆçº§è¶Šé«˜)\n")
    
    print("ğŸ“ çƒ­è¯æ–‡ä»¶æ ¼å¼:")
    print("  æ¯è¡Œä¸€ä¸ªçƒ­è¯,æ ¼å¼: çƒ­è¯ æƒé‡")
    print("  ç¤ºä¾‹: çº¢ç¯è·Ÿè½¦ 80\n")
    
    print("ğŸš€ åœ¨funasr-onnxä¸­ä½¿ç”¨çƒ­è¯:")
    print("  ç›®å‰funasr-onnxçš„SenseVoiceSmallç±»ä¸æ”¯æŒhotwordå‚æ•°")
    print("  å¦‚éœ€ä½¿ç”¨çƒ­è¯,å»ºè®®:")
    print("  1. ä½¿ç”¨æ”¯æŒçƒ­è¯çš„æ¨¡å‹(å¦‚Paraformer)")
    print("  2. æˆ–é€šè¿‡æœåŠ¡ç«¯éƒ¨ç½²æ–¹å¼ä½¿ç”¨çƒ­è¯\n")
    
    print("ğŸ” éªŒè¯çƒ­è¯æ•ˆæœ:")
    print("  é‡æ–°è¿è¡Œæ¨¡å‹å¯¹æ¯”æµ‹è¯•,è§‚å¯Ÿå‡†ç¡®ç‡æ˜¯å¦æå‡")
    print("  ç‰¹åˆ«å…³æ³¨ä¹‹å‰è¯†åˆ«é”™è¯¯çš„æ ·æœ¬\n")
    
    print(f"{'='*80}")
    print("çƒ­è¯ç”Ÿæˆå®Œæˆ!")
    print(f"{'='*80}\n")


if __name__ == '__main__':
    main()
